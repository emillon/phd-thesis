Dans ce chapitre, nous présentons un tour d'horizon des techniques existantes
permettant d'analyser des programmes. Un accent est mis sur la propriété de
sûreté décrite dans le chapitre~\ref{cha:os}, mais on ne se limite pas à
celle-ci.

L'analyse statique de programmes est un sujet de recherche actif depuis
l'apparition de la science informatique.

\section{Taxonomie}

\paragraph{Techniques statiques et dynamiques :} l'analyse peut être faite au
moment de la compilation, ou au moment de l'exécution. En général on peut
obtenir des informations plus précises de manière dynamique, mais cela ne prend
en compte que les parties du programme qui seront vraiment exécutées. Un autre
problème des techniques dynamiques est qu'il est souvent nécessaire
d'instrumenter l'environnement d'exécution (ce qui --- dans le cas où cela est
possible --- peut se traduire par un impact en performances). L'approche
statique, en revanche, nécessite de construire à l'arrêt une carte mentale du
programme, ce qui n'est pas toujours possible dans certains langages.

Dans la suite, nous considérerons essentiellement des techniques statiques,
précisant le contraire lorsque c'est nécessaire.

\paragraph{Cohérence et complétude :} le but d'une analyse statique est de
catégoriser les programmes selon leurs caractéristiques à l'exécution. Or,

\begin{theorem}[de Rice]
  Toute propriété non triviale sur le comportement dynamique des programmes est
  indécidable.\cite{rice}
\end{theorem}
% TODO formaliser un peu plus?

Autrement dit, il n'est pas possible d'écrire un analyseur statique parfait,
c'est à dire ne se trompant jamais. Toute technique statique va donc de se
retrouver dans au moins un des cas suivants :

\begin{itemize}
\item
  un programme valide est rejeté : on parle de \emph{faux positif}.
\item
  un programme invalide n'est pas détecté : on parle de
  \emph{faux négatif}.
\end{itemize}

En général on préfère s'assurer que les programmes acceptés possèdent la
propriété recherchée, quitte à en rejeter certains.

\section{Méthodes syntaxiques}

L'analyse la plus simple consiste à traiter un programme comme du texte, et à y
rechercher des motifs dangereux. Ainsi, utiliser des outils comme \texttt{grep}
permet parfois de trouver un grand nombre de vulnérabilités~\cite{SpenderGrep}.

On peut continuer cette approche en recherchant des motifs mais en étant
sensible à la syntaxe et au flot de contrôle du programme. Cette notion de
\emph{semantic grep} est présente dans l'outil Coccinelle
\cite{coccinelle09,coccinelle11} : on peut définir des
\emph{patches sémantiques} pour détecter ou modifier des constructions
particulières.

%TODO lire coccinelle09

\section{Interprétation abstraite}

L'interprétation abstraite est une technique d'analyse générique qui permet de
simuler statiquement tous les comportements d'un programme Cousot
\cite{Cousot77,Cousot92-1}. Un exemple d'application est de calculer les bornes
de variations des variables pour s'assurer qu'aucun débordement de tableau n'est
possible~\cite{AllamigeonHymansSSTIC07}. Cette technique est très puissante mais
possède plusieurs inconvénients. D'une part, pour réaliser une analyse
interprocédurale il faut partir d'un point en particulier du programme (comme la
fonction \texttt{main}). Cette hypothèse n'est pas facilement satisfaite dans un
noyau de système d'exploitation, qui possède de nombreux points d'entrée.

% TODO virer non?

\begin{figure}%{{{
\centering
\subbottom[
  L'ensemble des états d'erreur est calculable (zone hachurée)…
]{
\label{fig:ia-f1}
% 1
\begin{tikzpicture}[scale=0.7]%{{{
\clip  (-1,-1) rectangle (6,5);
%
% Hachures
\foreach \y in {-10,-9.3,...,11} \draw [gray, very thin, rotate=30] (-4,\y) to (18,\y);
%
% Ensemble sûr
\path [fill=white]
                 (0, 0) -- ++(0, 2)
            -- ++(1, 2) -- ++(3,-1)
            -- ++(1,-3) --    cycle;
%
% Ensemble sûr (redraw)
\draw            (0, 0) -- ++(0, 2)
            -- ++(1, 2) -- ++(3,-1)
            -- ++(1,-3) --    cycle;
%
\end{tikzpicture}%}}}
}
\hspace{5mm}
\subbottom[
  …mais pas celui des états atteints.
]{
\label{fig:ia-f2}
% 2
\begin{tikzpicture}[scale=0.7]%{{{
\clip  (-1,-1) rectangle (6,5);
%
% Hachures
\foreach \y in {-10,-9.3,...,11} \draw [gray, very thin, rotate=30] (-4,\y) to (18,\y);
%
% Ensemble sûr
\path [fill=white]
                 (0, 0) -- ++(0, 2)
            -- ++(1, 2) -- ++(3,-1)
            -- ++(1,-3) --    cycle;
%
% Ensemble sûr (redraw)
\draw            (0, 0) -- ++(0, 2)
            -- ++(1, 2) -- ++(3,-1)
            -- ++(1,-3) --    cycle;
%
% Comportement réel (sans erreurs)
\draw [fill=green!50,scale=0.8] plot[smooth cycle]
      coordinates{(1,1) (1,2) (2,3) (3,3) (3,1)};
%
\end{tikzpicture}%}}}
}

\subbottom[
  On construit donc une surapproximation calculable de la sémantique. Si
  celle-ci n'a pas d'erreur, on est assuré que le programme n'en aura pas non
  plus.
]{
\label{fig:ia-f3}
% 4
\begin{tikzpicture}[scale=0.7]%{{{
\clip  (-1,-1) rectangle (6,5);
%
% Hachures
\foreach \y in {-10,-9.3,...,11} \draw [gray, very thin, rotate=30] (-4,\y) to (18,\y);
%
% Ensemble sûr
\path [fill=white]
                 (0, 0) -- ++(0, 2)
            -- ++(1, 2) -- ++(3,-1)
            -- ++(1,-3) --    cycle;
%
% Ensemble sûr (redraw)
\draw            (0, 0) -- ++(0, 2)
            -- ++(1, 2) -- ++(3,-1)
            -- ++(1,-3) --    cycle;
%
% Approximation (précise)
\draw [fill=blue!50,scale around={1.3:(3,3)}]
           (1,1) -- (1,2)
        -- (2,3) -- (3,3)
        -- (3,1) -- cycle;
%
% Comportement réel (sans erreurs)
\draw [fill=green!50,scale=0.8] plot[smooth cycle]
      coordinates{(1,1) (1,2) (2,3) (3,3) (3,1)};
%
\end{tikzpicture}%}}}
}
\hspace{5mm}
\subbottom[
  Si on détecte une intersection, il n'est pas possible de savoir si elle est
  due à une erreur atteignable ou à une approximation trop laxe.
]{
\label{fig:ia-f4}
% 6
\begin{tikzpicture}[scale=0.7]%{{{
\clip  (-1,-1) rectangle (6,5);
%
% Hachures
\foreach \y in {-10,-9.3,...,11} \draw [gray, very thin, rotate=30] (-4,\y) to (18,\y);
%
% Ensemble sûr
\path [fill=white]
                 (0, 0) -- ++(0, 2)
            -- ++(1, 2) -- ++(3,-1)
            -- ++(1,-3) --    cycle;
%
% Ensemble sûr (redraw)
\draw            (0, 0) -- ++(0, 2)
            -- ++(1, 2) -- ++(3,-1)
            -- ++(1,-3) --    cycle;
%
% Approximation (imprécise)
\draw [fill=blue!50,scale around={1.3:(3,3)}]
            (1,1) -- (1,2)
         -- (3,4)
         -- (3,1) -- cycle;
%
% Comportement réel (sans erreurs)
\draw [fill=green!50,scale=0.8] plot[smooth cycle]
      coordinates{(1,1) (1,2) (2,3) (3,3) (3,1)};
%
\end{tikzpicture}%}}}
}

\caption
  [Surapproximation en interprétation abstraite]
{ Surapproximation en interprétation abstraite.
  Il n'est pas possible de déterminer si l'ensemble des états atteignables
  est inclus dans l'ensemble des états sûrs (figure~\ref{fig:ia-f2}). En
  revanche, en construisant une surapproximation on peut parfois conclure
  (figures~\ref{fig:ia-f3} et~\ref{fig:ia-f4}).
}
\label{fig:ia}

\end{figure}%}}}

Les domaines les plus simples ne capturent aucune relation entre variables. Ce
sont des domaines non relationnels. Par exemple le domaine des signes capture
uniquement le signe des variables (figure~\ref{fig:dom-sig}), et le domaine des
intervalles retient les bornes de variations extrémales des variables
(figure~\ref{fig:dom-intvl}).

\begin{figure}%{{{
\centering

\begin{minipage}{0.4\textwidth}
  \begin{tikzpicture}
  \node at (1, 2) (t) {$\top$};
  \node at (0, 1) (m) {$-$};
  \node at (1, 1) (z) {$0$};
  \node at (2, 1) (p) {$+$};
  \node at (1, 0) (b) {$\bot$};
  \draw (t) -- (p) -- (b);
  \draw (t) -- (m) -- (b);
  \draw (t) -- (z) -- (b);
  \end{tikzpicture}
\end{minipage}
\begin{minipage}{0.4\textwidth}
  \begin{align*}
  γ~(-) = & ℝ^- \\
  γ~(0) = & \{0\} \\
  γ~(+) = & ℝ^+ \\
  \end{align*}
\end{minipage}

\caption{Domaine des signes}
\label{fig:dom-sig}
\end{figure}%}}}

\begin{figure} % {{{
\centering
\begin{tikzpicture}
\node at (0,0)   (b)  {$\bot$};

\node at (-4,2) (cm2)   {$\{-2\}$};
\node at (-2,2) (cm1)   {$\{-1\}$};
\node at  (0,2)  (c0)   {$\{0\}$};
\node at  (2,2)  (c1)   {$\{1\}$};
\node at  (4,2)  (c2)   {$\{2\}$};

\node at (-3,3) (rm2m1) {[-2;-1]};
\node at (-1,3) (rm10)  {[-1;0]};
\node at  (1,3) (r01)   {[0;1]};
\node at  (3,3) (r12)   {[1;2]};

\node at (-2,4) (rm20)  {[-2;0]};
\node at  (0,4) (rm11)  {[-1;1]};
\node at  (2,4) (r02)   {[0;2]};

\node at (-1,5) (rm21)  {[-2;1]};
\node at  (1,5) (rm12)  {[-1;2]};

\node at  (0,6) (rm22)  {[-2;2]};

\node at  (0,9) (t)  {$\top$};

\draw (b) -- (cm2) -- (rm2m1) -- (rm20) -- (rm21) -- (rm22);
\draw (b) -- (cm1) -- (rm10) -- (rm11) -- (rm12);
\draw (b) -- (c0) -- (r01) -- (r02);
\draw (b) -- (c1) -- (r12);
\draw (b) -- (c2);

\draw (c2) -- (r12) -- (r02) -- (rm12) -- (rm22);
\draw (c1) -- (r01) -- (rm11) -- (rm21);
\draw (c0) -- (rm10) -- (rm20);
\draw (cm1) -- (rm2m1);

\draw[loosely dashed] (cm2)   to [bend left=30] (t);
\draw[loosely dashed] (rm2m1) to [bend left=30] (t);
\draw[loosely dashed] (rm20)  to [bend left=30] (t);
\draw[loosely dashed] (rm21)  to [bend left=30] (t);

\draw[loosely dashed] (rm22)  to (t);

\draw[loosely dashed] (rm12)   to [bend right=30] (t);
\draw[loosely dashed] (r02)   to [bend right=30] (t);
\draw[loosely dashed] (r12)   to [bend right=30] (t);
\draw[loosely dashed] (c2)    to [bend right=30] (t);

\draw[loosely dashed] (cm2) -- +(-1,0);
\draw[loosely dashed] (c2)  -- +(1,0);

\end{tikzpicture}

\caption{Domaine des intervalles}
\label{fig:dom-intvl}
\end{figure} % }}}

Lorsque plusieurs variables sont analysées en même temps, utiliser de tels
domaines consiste à considérer un produit cartésien d'ensembles abstraits
(figure~\ref{fig:dom-cartesien})

Cela revient à oublier les relations entre les variables. Des domaines abstraits
plus précis permettent de retenir celles-ci. Pour ce faire, il faut modéliser
l'ensemble des valeurs des variables comme un tout. Parmi les domaines
relationnels courants on peut citer :

\begin{itemize}

\item Le domaine des polyèdres, historiquement l'un des premiers
domaines relationnels. Il permet de retenir tous les invariants affines entre
fonctions (figure~\ref{fig:dom-poly}).

\item Le domaine des zones permet de représenter des relations affines de
forme $v_i - v_j \le c$ (figure~\ref{fig:dom-zones}).

\item Le domaine des octogones est un compromis entre les polyèdres et les
zones. Il permet de représenter les relations $\pm v_i \pm v_j \le c$
(figure~\ref{fig:dom-octo}).

\end{itemize}

\begin{figure}%{{{

  \centering

  %{{{
  \subbottom[Domaine non relationnel]{
    \label{fig:dom-cartesien}
      \begin{tikzpicture}
      \draw[->] (0,-0.5) -- (0,3);
      \draw[->] (-0.5,0) -- (4,0);
%
      \draw[fill=red!30] (1,1) rectangle (3,2);
%
      \draw[dashed] (3,1) -- (3,0);
      \draw[dashed] (1,1) -- (1,0);
%
      \draw[dashed] (1,2) -- (0,2);
      \draw[dashed] (1,1) -- (0,1);
%
      \draw[line width=3pt] (0,1) -- (0,2);
      \draw[line width=3pt] (3,0) -- (1,0);
%
      \end{tikzpicture}
  }%}}}
  \subbottom[Domaine des polyèdres]{%{{{
    \label{fig:dom-poly}
%
    \begin{tikzpicture}[scale=0.5]
    \path[use as bounding box] (0,0) rectangle (8, 6);
%
    \draw[fill=red!30] (1,2) -- (2,4) -- (4,5) -- (6,5) -- (7,3) -- (5,1) -- cycle;
%
    \end{tikzpicture}
  }%}}}

  \subbottom[Domaine des zones]{%{{{
    \label{fig:dom-zones}
%
    \begin{tikzpicture}
    \path[use as bounding box] (0,0) rectangle (4,4);
%
    \draw[fill=red!30] (1,2) -- (2,3) -- (3,3) -- (3,2) -- (2,1) -- (1,1) -- cycle;
%
    \end{tikzpicture}
  }%}}}
  \subbottom[Domaine des octaèdres]{%{{{
    \label{fig:dom-octo}
%
    \begin{tikzpicture}
    \path[use as bounding box] (0,0) rectangle (4,4);
%
    \draw[fill=red!30] (1,1.5) -- (1,2) -- (2,3) -- (2.5,3) -- (3,2.5) -- (3,2) -- (2,1) -- (1.5,1) -- cycle;
%
    \end{tikzpicture}
  }%}}}

  \caption{Quelques domaines abstraits}
  \label{fig:dom-abstraits}
\end{figure}%}}}

En plus des domaines numériques, il est nécessaire d'employer des domaines
spécialisés dans la modélisation de la mémoire. Cela est nécessaire pour pouvoir
"suivre" les pointeurs. Par exemple, on peut représenter un pointeur par un
ensemble de variables possiblement pointées et une valeur abstraite représentant
le décalage (\emph{offset}) du pointeur par rapport au début de la zone mémoire.
Cette valeur peut elle-même être abstraite par un domaine numérique.

Au delà des domaines eux-mêmes, l'analyse se fait sous forme d'un calcul de
point fixe. La manière la plus simple est d'utiliser un algorithme de
\emph{liste de travail}, décrit par exemple dans~\cite{tapsoft95}. Les
raffinement en revanche sont nombreux.

Dès~\cite{Cousot77} il est remarqué que la terminaison de l'analyse n'est
assurée que si le treillis des valeurs abstraites est de hauteur finie, ou qu'un
opérateur d'élargissement (\emph{widening}) $\nabla$ est employé. L'idée est
qu'une fois qu'on a calculé quelques termes d'une suite croissante, on peut
réaliser une projection de celle ci. Par exemple, dans le domaine des
intervalles, $[0;2]~\nabla~[0;3] = [0;+\infty[$. On atteint alors un point fixe
mais qui est plus grand que celui qu'on aurait obtenu sans cette accélération :
on perd en précision. Pour en gagner, on peut redescendre sur le treillis des
points fixe avec une suite d'itérations décroissantes \cite{granger}. Dans
l'itération de point fixe, il est possible d'obtenir les résultats de manière
plus efficace en choisissant un ordre particulier dans les calculs des
sous-itérations\cite{policy}.

% TODO pas clair

En termes d'ingéniérie logicielle, implanter un analyseur statique est un défi
en soi. En plus des domaines abstraits, d'un itérateur, il faut traduire le code
source à analyser dans un langage, et traduire les résultats de l'analyse en un
ensemble d'"alarmes" à présenter à l'utilisateur.

Pour des retours d'expérience, on peut se référer aux descriptions d'
Astrée~\cite{Astree04,Astree05,AstreeScale},
CGS~\cite{cgs},
ou Coverity~\cite{coverityBillion},

% TODO pas top

Une interprétation abstraite est par construction sûre et incomplète, donc ce
qui sépare une bon analyseur d'un mauvais est sa précision. Dans le cas du
langage C, de nombreuses constructions rendent imprécises les analyses :

% TODO c'est déplacé en c1

L'interprétation abstraite a été utilisée pour analyser le flot de données entre
objets~\cite{liang2012taint}.

Elle a l'inconvénient de remonter de nombreuses fausses alarmes, puisqu'elle
nécessite d'avoir une vue précise du programme. Elle est aussi plus adaptée aux
programmes qui n'ont qu'un point d'entrée.

\section{Typage}

\section{Analyse de code système}

Les logiciels système demandant des garanties de sécurité et de fiabilité, de
nombreuses analyses \emph{ad-hoc} ciblent les noyaux de systèmes d'exploitation.

Ajouter un système de types forts à C est l'idée centrale de
CCured~\cite{ccured-toplas}. Dans les cas où il n'est pas possible de conclure,
des vérifications à l'exécution sont ajoutées. Cependant, cela nécessite une
instrumentation dynamique qui est faite pour rester active, ce qui se paye en
performances.

Saturn~\cite{paste07} est un système pour analyser du code système écrit en C.
Il traite le problème des pointeurs utilisateur en utilisant une analyse de
forme ``pointe-sur''~\cite{oakland08}. Comme l'interprétation abstraite, son but
est d'être très précis, au détriment d'un temps de calcul important dans
certains cas.

\section{Logique de Hoare}

Une technique pour vérifier statiquement des propriétés sur la sémantique d'un
programme a été formalisée par Robert Floyd~\cite{FloydMeaning} et Tony
Hoare~\cite{hoare}.

Elle consiste à écrire les invariants qui sont maintenus à un point donné du
programme. Ces propositions sont écrites dans une logique $\mathcal{L}$.
Chaque instruction $i$ est annotée d'une pré-condition $P$
et d'une post-condition $Q$, ce que l'on note $\hoare{P}{i}{Q}$. Cela signifie
que si $P$ est vérifiée et que l'exécution de $i$ se termine
\footnote{
  Comme dans la plupart des cas, la vérification de la terminaison d'un
  algorithme est réalisée de manière séparée.
  % TODO faire un chapeau général façon Devie?
  % 1/ Terminaison
  % 2/ Correction
  % 3/ Complexité
}, alors $Q$ sera vérifiée.

En plus des règles de $\mathcal{L}$, des règles d'inférence traduisent la
sémantique du programme ; par exemple la règle de composition est :

\begin{mathpar}
  \irule{Hoare-Seq}
    { \hoare{P}{i_1}{Q} \\
      \hoare{Q}{i_2}{R}
    }{
      \hoare{P}{i_1;i_2}{R}
    }
\end{mathpar}

Les pré-conditions peuvent être renforcées et les post-conditions relâchées :

\begin{mathpar}
  \irule{Hoare-Consequence}
    { ⊢_{\mathcal{L}} P  ⇒ P' \\
      \hoare{P}{i}{Q} \\
      ⊢_{\mathcal{L}} Q' ⇒ Q
    }
    { \hoare{P'}{i}{Q'} }
\end{mathpar}

Il est alors possible d'annoter le programme avec ses invariants formalisés de
manière explicite dans $\mathcal{L}$. Ceux-ci seront vérifiés à la compilation.
% TODO à la compil?

La règle de conséquence permet de découpler les propriétés du programme lui-même
: plusieurs niveaux d'annotations sont possibles, du moins précis au plus
précis. En fait, il est même possible d'annoter chaque point de contrôle par
l'ensemble d'annotations vide : \hoare{T}{i}{T} est toujours vrai.

Augmenter graduellement les pré- et post-conditions est néanmoins assez
difficile, puisqu'il peut être nécessaire de modifier l'ensemble des conditions
à la fois.

Cette difficulté est mentionnée dans \cite{cssv}, où un système de programmation
par contrats est utilisé pour vérifier la correction de routines de manipulation
de chaînes en C.

Ce type d'annotations a été implanté par exemple pour le langage Java dans le
système JML\cite{jmlkluwer} ou pour le langage C\# dans Spec\#\cite{krml136}.

\section{Proposition}

% TODO

Le but de cette thèse est de montrer que les techniques de typage statique
permettent de vérifier que les pointeurs utilisateurs sont manipulés d'une
manière qui préserve l'intégrité du noyau d'un système d'exploitation.

% vim: spelllang=fr
