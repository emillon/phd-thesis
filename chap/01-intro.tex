Communication,
audiovisuel,
transports,
médecine:
tous ces domaines se sont transformés dans les dernières décennies,
en particulier grâce à la révolution numérique.
En effet le plus petit appareil électrique contient maintenant des composants
matériels programmables.

En 2014, on pense bien sûr aux téléphones portables dont la fonctionnalité et la
complexité les rapprochent des ordinateurs de bureau. Par exemple, le système
d'exploitation Android de Google est fondé sur le noyau Linux, destiné à la base
aux micro-ordinateurs.

Le noyau d'un système d'exploitation est chargé de faire l'intermédiaire entre
le matériel (processeur, mémoire, périphériques, …) et les applications
exécutées sur celui-ci (par exemple un navigateur web, une calculatrice ou un
carnet d'adresses).

Il doit aussi garantir la sécurité de celles-ci: en tant qu'intermédiaire de
confiance, le noyau a un certain nombre de responsabilités et est le seul à
avoir accès à certaines informations sensibles. Il est capital de s'assurer
qu'il est bien le seul à pouvoir y accéder. En particulier, il faut pouvoir
vérifier que les requêtes faites par l'utilisateur au noyau ne peuvent pas,
volontairement ou involontairement, détourner ce dernier et lui faire fuiter des
informations confidentielles.

Le problème est que, comme tous les logiciels, les noyaux de système
d'exploitation sont écrits par des humains qui ne sont pas parfaits. Les
activités de relecture et de débogage ont beau prendre la majeure partie du
temps de développement, il est facile de laisser passer des défauts de
programmation.

Ces erreurs, ou \emph{bugs}, peuvent avoir des conséquences dramatiques sur le
plan matériel ou humain. À titre d'exemple, un Airbus A320 embarque près de 10
millions de lignes de code: il est capital de vérifier que celles-ci ne peuvent
pas mettre en danger la sûreté des passagers.

Une technique efficace est de réaliser des tests, c'est-à-dire exécuter le
programme sous un environnement contrôlé. On peut alors détecter des
comportements non désirés. Mais même avec une grande quantité de tests il n'est
pas possible de couvrir tous les cas d'utilisation.

Une autre approche est d'analyser le code source du programme avant de
l'exécuter et de refuser de lancer les programmes qui contiennent certaines
constructions dangereuses. C'est l'analyse statique de programmes.

Une des techniques d'analyse statique les plus répandues et les plus simples est
le typage statique, qui consiste à associer, à chaque morceau de programme, une
étiquette décrivant quel genre de valeur sera produite par son évaluation. Par
exemple, si $n$ est le nom d'une variable entière, alors $n + 2$ produira
toujours une valeur entière.
Cela permet de savoir si les programmes manipuleront des données incompatibles
entre elles.

Pour en revenir aux noyaux de système d'exploitation, ceux-ci manipulent à la
fois des données sensibles et des données provenant du monde extérieur, pour
lesquelles on n'a aucune garantie. On veut pouvoir distinguer ces deux genres de
données.

Plus précisément, un des points cruciaux pour garantir l'isolation d'un noyau de
système d'exploitation est de restreindre la manière dont sont traitées les
informations provenant des programmes utilisateur.

Le but de cette thèse est de montrer que le typage statique peut être utilisé
pour détecter et interdire ces manipulations dangereuses.

\section{Rôle d'un système d'exploitation}

Un ordinateur est constitué de nombreux composants matériels: microprocesseur,
mémoire, et divers périphériques. Et au niveau de l'utilisateur, des dizaines de
logiciels permettent d'effectuer toutes sortes de calculs et de communications.
Le système d'exploitation permet de faire l'interface entre ces aspects.

Au cours de l'histoire des systèmes informatiques, la manière de les programmer
a beaucoup évolué. Au départ, les programmeurs avaient accès au matériel dans
son intégralité: toute la mémoire pouvait être accédée, toutes les instructions
pouvaient être utilisées.

Néanmoins c'est un peu restrictif, puisque cela ne permet qu'à une personne
d'interagir avec le système. Dans la seconde moitié des années 1960, sont
apparus les premiers systèmes \enquote{à temps partagé}, permettant à plusieurs
utilisateurs de travailler en même temps.

Permettre l'exécution de plusieurs programmes en même temps est une idée
révolutionnaire, mais elle n'est pas sans difficultés techniques: en effet les
ressources de la machine doivent être aussi partagées entre les utilisateurs et
les programmes. Par exemple, plusieurs programmes vont utiliser le processeur
les uns à la suite des autres; et chaque programme aura à sa disposition une
partie de la mémoire principale, ou du disque dur.

Si plusieurs programmes s'exécutent de manière concurrente sur le même matériel,
il faut s'assurer que l'un ne puisse pas écrire dans la mémoire de l'autre, et
aussi que les deux n'utilisent pas la carte réseau en même temps. Ce sont des
rôles du système d'exploitation.

Ainsi, au lieu d'accéder directement au matériel via des instructions de bas
niveau, les programmes communiquent avec le noyau, qui centralise donc les
appels au matériel, et abstrait certaines opérations.

Par exemple, comparons ce qui se passe concrètement lors de la copie de données
depuis un cédérom ou une clef USB.\@

\begin{itemize}

  \item Dans le cas du cédérom, il faut interroger le bus SATA, interroger le
    lecteur sur la présence d'un disque dans le lecteur, activer le moteur,
    calculer le numéro de trame des données sur le disque, demander la lecture,
    puis déclencher une copie de la mémoire.

  \item Avec une clef, il faut interroger le bus USB, rechercher le bon numéro
    de périphérique, le bon numéro de canal dans celui-ci, lui appliquer une
    commande de lecture au bon numéro de bloc, puis copier la mémoire.

\end{itemize}

Ces deux opérations, bien qu'elles aient la même intention (copier de la mémoire
depuis un périphérique amovible), ne sont pas effectuées en extension de la même
manière. C'est pourquoi le système d'exploitation fournit les notions de
fichier, lecteur, etc: le programmeur n'a plus qu'à utiliser des commandes de
haut niveau (\enquote{monter un lecteur}, \enquote{ouvrir un fichier},
\enquote{lire dans un fichier}) et, selon le type de lecteur, le système
d'exploitation effectuera les actions appropriées.

En résumé, un système d'exploitation est l'intermédiaire entre le logiciel et le
matériel, et en particulier est responsable de la gestion de la mémoire, des
périphériques et des processus. Les détails d'implantation ne sont pas présentés
à l'utilisateur; à la place, il manipule des abstractions, comme la notion de
fichier. Pour une explication détaillée du concept de système d'exploitation
ainsi que des cas d'étude, on pourra se référer à~\cite{tanenbaum}.

\section{Séparation entre noyau et espace utilisateur}

Puisque le noyau est garant du bon fonctionnement du système, il ne doit pas
pouvoir être manipulé directement par l'utilisateur ou les programmes exécutés.
Ainsi, il est nécessaire de mettre en place des protections entre les espaces
noyau et utilisateur.

Au niveau matériel, on utilise la notion de \emph{niveaux de privilèges} pour
déterminer s'il est possible d'exécuter une instruction.

D'une part, le processeur contient un niveau de privilège intrinsèque. D'autre
part, \linebreak chaque zone mémoire contenant du code ou des données possède
également un niveau de privilège minimum nécessaire. L'exécution d'une
instruction est alors possible si et seulement si le niveau de privilège du
processeur est supérieur à celui de l'instruction et des opérandes mémoires qui
y sont présentes
\footnote{
  Ici \enquote{supérieur} est synonyme de \enquote{plus privilégié}. Dans l'implantation
  d'Intel présentée dans le chapitre~\ref{cha:os}, les niveaux sont numérotés de
  0 à 3, où le niveau 0 est le plus privilégié.
}.

Par exemple, supposons qu'un programme utilisateur contienne l'instruction
\enquote{déplacer le contenu du registre \eax{} vers l'adresse mémoire
\texttt{a}}, où \texttt{a} fait partie de l'espace mémoire de l'utilisateur.
Alors aucune erreur de protection mémoire n'est déclenchée.

Ainsi, pour une instruction manipulant des données en mémoire, les accès
possibles sont décrits dans le tableau suivant. En cas d'impossibilité (signalée
par un \Square), une erreur se produit et l'exécution s'arrête. Par exemple,
l'avant-dernière ligne indique que, si un programme tente de lire une variable du
noyau, celui-ci sera arrêté par une exception.

\begin{center}
\def\modeK{Noyau\xspace}
\def\modeU{Utilisateur\xspace}
\begin{tabular}{cccc}
\toprule
  Mode du processeur
& Privilège (code)
& Privilège (données)
& Accès possible \\
\midrule
  \modeK{} & \modeK{} & \modeK{} & \CheckedBox{} \\
  \modeK{} & \modeK{} & \modeU{} & \CheckedBox{} \\
  \modeK{} & \modeU{} & \modeK{} & \CheckedBox{} \\
  \modeK{} & \modeU{} & \modeU{} & \CheckedBox{} \\
  \modeU{} & \modeK{} & \modeK{} & \Square{} \\
  \modeU{} & \modeK{} & \modeU{} & \Square{} \\
  \modeU{} & \modeU{} & \modeK{} & \Square{} \\
  \modeU{} & \modeU{} & \modeU{} & \CheckedBox{} \\
\bottomrule
\end{tabular}
\end{center}
\label{page:erreursec}

En plus de cette vérification, certains types d'instructions sont explicitement
réservés au mode le plus privilégié: par exemple les lectures ou écritures sur
des ports matériels, ou celles qui permettent de définir les niveaux de
privilèges des différentes zones mémoire.

Comme les programmes utilisateur ne peuvent pas accéder à ces instructions de
bas niveau, ils sont très limités dans ce qu'ils peuvent faire. En utilisant
seulement les seules instructions non privilégiées, on peut uniquement réaliser
des calculs, sans réaliser d'opérations visibles depuis l'extérieur du
programme.

Pour utiliser le matériel ou accéder à des abstractions de haut niveau (comme
créer un nouveau processus), ils doivent donc passer par l'intermédiaire du
noyau. La communication entre le noyau et les programmes utilisateur est
constituée par le mécanisme des \emph{appels système}.

Lors d'un appel système, une fonction du noyau est invoquée (en mode noyau) avec
des paramètres provenant de l'utilisateur. Il faut donc être particulièrement
précautionneux dans le traitement de ces données.

Par exemple, considérons un appel système de lecture depuis un disque: on passe
au noyau les arguments $(d, o, n, a)$ où $d$ est le nom du disque, $o$ (pour
\emph{offset}) l'adresse sur le disque où commencer la lecture, $n$ le nombre
d'octets à lire et $a$ l'adresse en mémoire où commencer à stocker les
résultats.

Dans le cas d'utilisation prévu, le noyau va copier la mémoire lue dans $a$. Le
processeur est en mode noyau, en train d'exécuter une instruction du noyau
manipulant des données utilisateur. D'après le tableau de la
page~\pageref{page:erreursec}, aucune erreur ne se produit.

Mais même si ce cas ne produit pas d'erreur à l'exécution, il est tout de même
codé de manière incorrecte. En effet, si on passe à l'appel système une adresse
$a$ faisant partie de l'espace noyau, que se passe-t-il?

L'exécution est presque identique: au moment de la copie on est en mode noyau,
en train d'exécuter une instruction du noyau manipulant des données noyau.
Encore une fois il n'y a pas d'erreur à l'exécution.

On peut donc écrire n'importe où en mémoire. De même, une fonction d'écriture
sur un disque (et lisant en mémoire) permettrait de lire de la mémoire du noyau.
À partir de ces primitives, on peut accéder aux autres processus exécutés, ou
détourner l'exécution vers du code arbitraire. L'isolation est totalement
brisée à cause de ces appels système.

La cause de ceci est qu'on a accédé à la mémoire en testant les privilèges du
noyau au lieu de tester les privilèges de celui qui a fait la requête
(l'utilisateur). Ce problème est connu sous le nom de \emph{confused deputy
problem}~\cite{hardy88confused}.

Pour implanter un appel système, il est donc nécessaire d'interdire le
déréférencement direct des pointeurs dont la valeur peut être contrôlée par
l'utilisateur. Dans le cas du passage par adresse d'un argument, il aurait fallu
vérifier à l'exécution que celui-ci a bien les mêmes privilèges que l'appelant.

Il est facile d'oublier d'ajouter cette vérification, puisque le cas
\enquote{normal} fonctionne. Avec ce genre d'exemple on voit comment les bugs
peuvent arriver si fréquemment et pourquoi il est aussi capital de les détecter
avant l'exécution.

\section{Systèmes de types}
\label{sec:types-intro}

La plupart des langages de programmation incorporent la notion de type, dont un
des buts est d'empêcher de manipuler des données incompatibles entre elles.

En mémoire, les seules données qu'un ordinateur manipule sont des nombres. Selon
les opérations effectuées, ils seront interprétés comme des entiers, des
adresses mémoire ou des caractères. Pourtant il est clair que certaines
opérations n'ont pas de sens: par exemple, multiplier un nombre par une adresse
ou déréférencer le résultat d'une division sont des comportements qu'on voudrait
pouvoir empêcher.

En un mot, le but du typage est de classifier les objets et de restreindre les
opérations possibles selon la classe d'un objet: en somme, \enquote{ne pas
ajouter des pommes et des oranges}. Le modèle qui permet cette classification
est appelé \emph{système de types} et est en général constitué d'un ensemble de
\emph{règles de typage}, comme \enquote{un entier plus un entier égale un
entier}.

\paragraph{Typage dynamique}

Dans ce cas, chaque valeur manipulée par le programme est décorée d'une
étiquette définissant comment interpréter la valeur en question. Les règles de
typage sont alors réalisées à l'exécution. Par exemple, l'opérateur
\enquote{$+$} vérifie que ses deux opérandes ont une étiquette \enquote{entier},
et construit alors une valeur obtenue en faisant l'addition des deux valeurs,
avec une étiquette \enquote{entier}. Par exemple, le langage
Python~\link{python} utilise cette stratégie.

\paragraph{Typage statique}

Dans ce cas on fait les vérifications à la compilation. Pour vérifier ceci, on
donne à chaque fonction un contrat comme \enquote{si deux entiers sont passés,
et que la fonction renvoie une valeur, alors cette valeur sera un entier}. Cet
ensemble de contrats peut être vérifié statiquement par le compilateur, à l'aide
d'un système de types statique.

Par exemple, on peut dire que l'opérateur \enquote{$+$} a pour type $(\tInt,
\tInt) → \tInt$. Cela veut dire que, si on lui passe deux entiers ($\tInt$
\tInt), alors la valeur obtenue est également un entier. \emph{A contrario}, si
autre chose qu'un entier est passé à cet opérateur, le programme ne compile pas.

\paragraph{Typage fort ou faible}

Indépendamment du moment où est faite cette analyse, on peut avoir plus ou moins
de garanties sur les programmes sans erreurs de typage. En poussant à l'extrême,
les systèmes de types forts garantissent que les valeurs ont toujours le type
attendu. Avec du typage statique, cela permet d'éliminer totalement les tests de
typage à l'exécution. Mais souvent ce n'est pas le cas, car il peut y avoir des
constructions au sein du langage qui permettent de contourner le système de
types, comme un opérateur de transtypage. On parle alors de typage faible.

\paragraph{Polymorphisme}

Parfois, il est trop restrictif de donner un unique type à une fonction. Si on
considère une fonction ajoutant un élément à une liste, ou une autre
triant un tableau en place, leur type doit-il faire intervenir le type des
éléments manipulés?

En première approximation, on peut imaginer fournir une version du code par type
de données à manipuler. C'est la solution retenue par les premières versions du
langage Pascal, ce qui rendait très difficile l'écriture de
bibliothèques~\cite{PascalNoEscape}. On parle alors de monomorphisme.

Une autre manière de procéder est d'autoriser plusieurs fonctions à avoir le
même nom, mais avec des types d'arguments différents. Par exemple, on peut
définir séparément l'addition entre deux entiers, entre deux flottants, ou entre
un entier et un flottant. Selon les informations connues à la compilation, la
bonne version sera choisie. C'est ainsi que fonctionnent les opérateurs en C++.
On parle de polymorphisme \emph{ad hoc}, ou de surcharge.

Une autre technique est de déterminer la fonction appelée non pas par le type de
ses arguments, mais par l'objet sur lequel on l'appelle. Cela permet d'associer
le comportement aux données. On parle alors de polymorphisme objet. Dans ce cas,
celui-ci repose sur le sous-typage: si $A_1$ et $A_2$ sont des sous-types de
$B$, on peut utiliser des valeurs de type $A_1$ ou $A_2$ là où une valeur de
type $B$ est attendue. Dans ce cas, la fonction correspondante sera appelée.

La dernière possibilité est le polymorphisme paramétrique, qui consiste à
utiliser le \linebreak même code quel que soit le type des arguments. Dans ce
cas, on utilise une seule fonction pour traiter une liste d'entiers ou une liste
de flottants, par exemple. Au lieu d'associer à chaque fonction un type, dans
certains cas on lui associe un type paramétré, instanciable en un type concret.
Dans le cas des fonctions de traitement de liste, l'idée est que lorsqu'on ne
touche pas aux éléments, alors le traitement est valable quel que soit leur
type. Cette technique a été décrite en premier dans~\cite{Milner78}.

Pour un tour d'horizon de différents systèmes de types statiques, avec en
particulier du polymorphisme, on pourra se référer à~\cite{TAPL}.

\section{Langages}
\label{sec:lang-int}

Le système Unix, développé à partir de 1969, a tout d'abord été développé en
assembleur sur un mini-ordinateur PDP-7, puis a été porté sur d'autres
architectures matérielles. Pour aider ce portage, il a été nécessaire de créer
un \enquote{assembleur portable}, le langage C~\cite{KandR,AnsiC}. Son but est
de fournir des abstractions au dessus du langage d'assemblage. Les structures de
contrôle (\texttt{if}, \texttt{while}, \texttt{for}) permettent d'utiliser la
programmation structurée, c'est-à-dire en limitant l'utilisation de
l'instruction \texttt{goto}. Les types de données sont également abstraits de la
machine: ainsi, \texttt{int} désigne un entier machine, indépendamment de sa
taille concrète. Son système de types, bien que statique (il peut y avoir des
erreurs de typage à la compilation), est assez rudimentaire: toutes les formes
de transtypage sont acceptées, certaines conversions sont insérées
automatiquement par le compilateur, et la plupart des abstractions fournies par
le langage sont perméables. Le noyau Linux est écrit dans un dialecte du langage
C. Le noyau du système Mac OS X d'Apple est également un dérivé d'Unix, et est
donc aussi écrit dans ce langage.

Néanmoins ce langage n'est pas facile à analyser, car il est conçu pour être
facilement écrit par des programmeurs humains. Certaines constructions sont
ambigües\footnote{
    Selon qu'il existe un type nommé \texttt{a}, l'expression \texttt{(a)-(b)}
    sera interprétée comme le transtypage de \texttt{-(b)} dans le type
    \texttt{a}, ou la soustraction des deux expressions \texttt{(a)} et
    \texttt{(b)}.
}, et de nombreux comportements sont implicites.

Si on veut analyser des programmes, il est plus pratique de travailler sur une
représentation intermédiaire plus simple afin d'avoir moins de traitements
dupliqués. Dans ce cas on ajoute une phase préliminaire à l'analyse, qui
consiste à convertir le code à étudier vers cette représentation. On présente
quelques candidats langages qui peuvent servir ce rôle:

\subsection*{Middle-ends}

Les premiers candidats sont bien entendu les représentations intermédiaires
utilisées dans les compilateurs C. Elles ont l'avantage d'accepter, en plus du C
standard, les diverses extensions (GNU, Microsoft, Plan9) utilisées par la
plupart des logiciels. En particulier, le noyau Linux repose fortement sur les
extensions GNU.\@

\paragraph{GCC} utilise une représentation interne nommée
GIMPLE~\cite{gcc-gimple}. Il s'agit d'une structure d'arbre écrite en C,
reposant sur de nombreuses macros afin de cacher les détails d'implantation
interne pouvant varier entre deux versions. Cette représentation étant réputée
difficile à manipuler, le projet MELT~\cite{gcc-melt} permet de générer un
greffon de compilateur écrit dans un dialecte de Lisp.

\paragraph{LLVM}\cite{llvm-pres} est un compilateur développé par la communauté
\emph{open-source} puis sponsorisé par Apple. À la différence de GCC, sa base de
code est écrite en C++. Il utilise une représentation intermédiaire qui peut
être manipulée sous forme d'une structure de données C++, d'un fichier de
code-octet compact, ou textuelle.

\paragraph{Cmm} est une représentation interne utilisée pour la génération de
code lors de la compilation d'OCaml~\link{ocaml}, et disponible dans les sources
du compilateur (il s'agit donc d'une structure de données OCaml). Ce langage a
l'avantage d'être très restreint, mais malheureusement il n'existe pas
directement de traducteur permettant de compiler C vers Cmm.

\paragraph{C-~-}\cite{spjcmm} \link{cmm}, dont le nom est inspiré du précédent,
est un projet qui visait à unifier les langages intermédiaires utilisés par les
compilateurs. L'idée est que, si un front-end peut émettre du C-~- (sous forme de
texte), il est possible d'obtenir du code machine efficace. Le compilateur
Haskell GHC, par exemple, utilise une représentation intermédiaire très
similaire à C-~-.

\subsection*{Langages intermédiaires ad hoc}

Comme le problème de construire une représentation intermédiaire adaptée à une
analyse statique n'est pas nouveau, plusieurs projets ont déjà essayé d'y
apporter une solution. Puisqu'ils sont développés en parallèle des compilateurs,
le support des extensions est en général moins important dans ces langages.

% TODO l'un ou l'autre?
\paragraph{CIL}\cite{NeculaCil} \link{cil} est une représentation en
OCaml d'un programme C, développée depuis 2002. Grâce à un mécanisme de
greffons, elle permet de prototyper rapidement des analyses statiques de
programmes.

\paragraph{CompCert} est un projet qui vise à produire un compilateur certifié
pour C. C'est-à-dire que les transformations sémantiques sont faites de manière
prouvée. Il utilise de nombreux langages intermédiaires, dont Compcert C,
Clight~\cite{cfront} et Cminor~\cite{cminorSLfull} qui est utilisé pour les
passes de middle-end.

\section{Le projet Penjili}

En face de ce problème théorique et technique, il faut mettre en perspective les
problématiques industrielles liées à celui-ci. Les travaux présentés ici ont en
effet été réalisés dans l'équipe de sécurité et sûreté logicielle d'EADS
Innovation Works, dans le cadre d'une convention industrielle de formation par
la recherche (CIFRE).

Aujourd'hui, la réussite de missions dépend de logiciels de taille de plus en
plus importante. Ainsi en cas de fautes dans ce genre de logiciel, on peut se
retrouver face à grands impacts économiques, voire risquer des vies humaines. On
comprend bien que les phases de vérification et de certification sont au cœur
du cycle de vie des logiciels avioniques. A titre d'exemple, l'échec du premier
vol d'Ariane 5 aurait certainement pu être évité si le logiciel de contrôle de
vol avait été vérifié plus efficacement~\cite{Ariane501}.

Plusieurs méthodes existent pour éliminer les risques de fautes. En fait, deux
approches duales sont nécessaires. La première consiste à mettre le logiciel
dans des situations concrètes et à vérifier que la sortie correspond au résultat
attendu : c'est la technique des tests.  Les tests \enquote{boîte noire}
consistent à tester en ayant à disposition uniquement les spécifications des
modules à plusieurs échelles. L'application est découpée en \enquote{boîtes} à
plusieurs échelles (par exemple : logiciel, module, classe, méthode). Au
contraire, les tests dits \enquote{boîte blanche} sont écrits en ayant à
disposition l'implémentation. Cela permet par exemple de s'assurer que chaque
chemin d'exécution est emprunté. Cette manière de procéder est similaire à la
preuve par neuf enseignée aux enfants : il est possible de prouver l'erreur,
mais pas que le programme est correct.

L'approche des méthodes formelles, au contraire, permet de s'assurer de
l'absence d'erreurs à l'exécution. Par exemple, l'analyse statique par
interprétation abstraite permet d'étudier les relations exposées entre les
variables afin d'en déduire les domaines dans lesquels elles évoluent. En
s'assurant que ceux-ci sont \enquote{sûrs}, on prouve l'absence d'erreurs de
manière automatisée.

L'analyse statique par interprétation abstraite se place à un niveau comparable
à celui des tests unitaires et fonctionnels, c'est-à-dire très près du code. Le
but est en particulier d'éliminer les erreurs au niveau du langage : division
par zéro, déréférencement d'un pointeur nul ou invalide, débordement
arithmétique sur les entiers, débordement de tableau, etc.

En pratique, dans l'embarqué les règles de codage limitent les constructions
posant problème. Par exemple, les exceptions, l'allocation dynamique
(\texttt{malloc} / \texttt{free}), l'instruction \texttt{goto} et les fonctions
récursives.

L'interprétation abstraite repose sur l'idée suivante : au lieu de considérer
que les variables possèdent une valeur, on utilise un domaine abstrait qui
permet de voir les variables comme possédant un ensemble de valeurs possibles.

\begin{figure}
\centering

\tikzstyle{patA}=[fill=white]
\tikzstyle{patB}=[fill=black!30]
\tikzstyle{patC}=[fill=white,pattern=crosshatch dots]

\begin{tikzpicture}[scale=0.7]
\clip  (-1,-1) rectangle (12,12);

% Hachures
\foreach \y in {-10,-9.3,...,11} \draw [gray, very thin, rotate=30] (-4,\y) to (18,\y);

% Ensemble sûr
\foreach \xsh in {0, 180}
\foreach \ysh in {0, 180}
\path [xshift=\xsh,yshift=\ysh,patA]
                 (0, 0) -- ++(0, 2)
            -- ++(1, 2) -- ++(3,-1)
            -- ++(1,-3) --    cycle;

% Approximation (précise)
\foreach \ysh in {0,180}
\draw [yshift=\ysh,patB,scale around={1.3:(3,3)}]
                (1,1) -- (1,2)
             -- (2,3) -- (3,3)
             -- (3,1) -- cycle;

% Approximation (imprécise)
\foreach \ysh in {0,180}
\draw [xshift=180,yshift=\ysh,patB,scale around={1.3:(3,3)}]
                (1,1) -- (1,2)
              -- (4,5)
             -- (4,1) -- cycle;

% Comportement réel (sans erreurs)
\foreach \xsh in {0, 180}
\draw [xshift=\xsh,yshift=180,patC,scale=0.8] plot[smooth cycle]
      coordinates{(1,1) (1,2) (2,3) (3,3) (3,1)};

% Comportement réel (avec erreurs)
\foreach \xsh in {0, 180}
\draw [xshift=\xsh,patC,scale=0.8] plot[smooth cycle]
      coordinates{(1,1) (1,2) (2,3) (3,3) (4.5,5) (3,1)};

% Ensemble sûr (redraw)
\foreach \xsh in {0, 180}
\foreach \ysh in {0, 180}
\draw [xshift=\xsh,yshift=\ysh]
                 (0, 0) -- ++(0, 2)
            -- ++(1, 2) -- ++(3,-1)
            -- ++(1,-3) --    cycle;

\end{tikzpicture}
\caption{Surapproximation. L'ensemble des états erronés est hachuré. L'ensemble
des états effectifs du programme, noté par des points, est surapproximé par
l'ensemble en gris.}
\label{fig:staticanalysis}
\end{figure}

Le domaine \enquote{sûr} a une forme assez simple compte tenu des erreurs
considérées: c'est un produit d'ensembles simples, comme des intervalles.
L'ensemble des comportements réels du programme est au contraire d'une forme
plus complexe.

En calculant une approximation de ce dernier, de forme plus simple, on peut
tester plus facilement que les comportements de sont dans la zone sûre : le fait
que l'analyse soit \emph{sound}, c'est-à-dire que l'approximation ne manque
aucun comportement, permet de prouver l'absence d'erreurs.

La figure~\ref{fig:staticanalysis} résume cette approche : l'ensemble des
valeurs dangereuses est représenté par un ensemble hachuré, l'ensemble des
comportements réels du programme est noté par des points, et l'approximation en
gris. Plusieurs cas peuvent se produire. Ils correspondent aux cas suivants, de
haut en bas puis de gauche à droite. Dans le premier on a prouvé, à la
compilation, que le programme ne pourra pas comporter d'erreurs à l'exécution.
Dans le deuxième, l'approximation recouvre les cas dangereux: on émet une alarme
par manque de précision. Dans la troisième l'approximation n'est pas
\emph{sound} (par construction, on évite ce cas). Enfin, dans la quatrième, on
émet une alarme à raison car il existe des comportements erronés. Toute la
difficulté est donc de construire une surapproximation correcte mais conservant
une précision suffisante.

Pour construire cette surapproximation, on peut employer divers outils. Par
exemple, un entier pourra être représenté par sa valeur minimale et sa valeur
maximale (domaine abstrait des intervalles), et un pointeur sur un tableau peut
être représenté par un ensemble de variables associé à un décalage
(\emph{offset}) par rapport au début de la zone mémoire (domaine des pointeurs
sur tableaux).

%\begin{figure}
%\insertcode{strlen.c}
%\caption{Implantation de la fonction \texttt{strlen}}
%\label{fig:strlen}
%\end{figure}

%Il est aussi important de repérer les relations qui peuvent exister entre les
%variables. Par exemple, dans la figure~\ref{fig:strlen}, les variables
%\texttt{p} et \texttt{i} sont liées dans la boucle: le décalage du pointeur
%\texttt{p} est égal à la somme du décalage de \texttt{str} et de \texttt{i}.

%Repérer ces relations est avantageux car il peut permettre de n'activer que les
%domaines abstraits nécessaires. Mais cela peut être coûteux, surtout lorsque le
%programme comporte un grand nombre de variables.

Dans ce sens, des outils fondés sur l'interprétation abstraite ont été
développés chez EADS Innovation Works dans le cadre du projet
Penjili~\cite{AllamigeonHymansSSTIC07}.

Les analyses statiques ne manipulent pas directement du code C, mais un langage
intermédiaire appelé \newspeak~\cite{newspeak}. Celui-ci est suffisamment
expressif pour compiler la plupart des programmes C, y compris de nombreuses
extensions GNU utilisées dans le noyau Linux (section~\ref{sec:gnuc}), et des
traducteurs automatiques depuis C et Ada existent (section~\ref{sec:compil}).

Ensuite, ses instructions sont orthogonales et minimales: il existe en général
une seule manière de faire les choses. Par exemple, le flot de contrôle est
restreint à la boucle infinie et au saut en avant (\enquote{\emph{break}}
généralisé).

Enfin, lorsque certaines constructions sont ambigües, un choix est fait. Par
exemple, l'évaluation des arguments d'une fonction est faite de manière
explicite, les tailles des types sont indiquées à chaque déclaration de
variable, etc.

Séparer le langage intermédiaire de la phase d'analyse permet de beaucoup
simplifier l'analyseur statique. D'une part, les constructions redondantes comme
les différents types de boucles ne sont traitées qu'une fois. D'autre part,
lorsque le langage source est étendu (en supportant une nouvelle extension de C
par exemple), l'analyseur n'a pas besoin d'être modifié.

Ces outils sont disponibles sous license libre sur~\link{penjili}. L'analyseur
statique Penjili, reposant sur ces outils, a été utilisé pour analyser des
logiciels embarqués critiques de plusieurs millions de lignes de code. Ce
dernier n'est pour le moment pas \emph{open-source}. Tous ces outils sont écrits
dans le langage OCaml~\cite{DAOC}.

\section{De l'avionique à l'informatique d'entreprise}

Vérifier la sûreté des logiciels avioniques est critique, mais cela présente
l'avantage que ceux-ci sont développés avec ces difficultés à l'esprit. Il est
plus simple de construire un système sécurisé en connaissant toutes les
contraintes d'abord, plutôt que de vérifier \emph{a posteriori} qu'un système
existant peut répondre à ces contraintes de sûreté.

Néanmoins cette conception de ces logiciels est très coûteuse. Pour des
composants qui sont moins critiques, il peut donc être intéressant de considérer
des logiciels ou bibliothèques existantes, en particulier dans le monde de
l'\emph{open-source}.

Ces logiciels sont plus difficiles à analyser car ils sont écrits sans
contraintes particulières. Non seulement toutes les constructions du langage
sont autorisées, mêmes celles qui sont difficiles à traiter (transtypage,
allocation dynamique, récursion, accès au système de fichiers, etc), mais aussi
des extensions non standards peuvent être utilisées.

%\paragraph{Problèmes liés à la compilation}

%\begin{figure}%{{{
    %\centering
    %\insertcode{bcopy.c}

    %\caption{Utilisation d'alternatives lors de la compilation}
    %\label{fig:bcopy}
%\end{figure}%}}}

%La compilation d'un programme est la suite d'opérations qui permet d'engendrer
%un fichier exécutable à partir de l'ensemble du code source de celui-ci.
%Habituellement cela se fait selon la suite d'opérations suivantes. Tout d'abord,
%l'environnement de compilation est détecté. Cela permet d'ajuster certaines
%variables de configuration. Par exemple, si le code source utilise la fonction
%\texttt{memmove} et que celle-ci n'est pas disponible dans l'environnement de
%compilation, il est possible d'utiliser \texttt{bcopy} à la place
%(figure~\ref{fig:bcopy}).

%Ensuite, peut venir une étape de génération de code source. Cela arrive dans le
%cas ou certaines données calculables statiquement doivent être disponible à
%l'exécution, ou dans le cas d'utilisation d'outils comme \texttt{lex} et
%\texttt{yacc} qui permettent d'engendrer des analyseurs lexicaux et syntaxiques.

%Puis vient l'étape de compilation en elle-même. Chaque fichier source est
%prétraité à l'aide de \texttt{cpp}, puis est compilé (en général via
%\texttt{gcc}) en code-objet (fichier d'extension \texttt{.o}).

%Enfin, la phase d'édition de liens permet de grouper ces fichiers en un
%exécutable. Des références vers les bibliothèques dynamiques nécessaires sont
%placées dans cet exécutable, et seront résolues au lancement de celui-ci.

%Pour analyser le code source, le plus simple est de se \enquote{brancher} à ce
%système de compilation, par exemple en créant une commande nommée \texttt{gcc}
%qui transmet le code source à l'analyseur statique. Il est à remarquer que les
%fichiers sont alors vus un par un, ce qui n'est pas compatible avec toutes les
%analyses.

\paragraph{Programmes non autosuffisants}

La grande majorité des programmes ne se suffisent pas à eux-mêmes. En effet, ils
interagissent presque toujours avec leur environnement ou appellent des
fonctions de bibliothèque.

Cela veut dire qu'un fichier en cours d'analyse peut contenir des appels à des
fonctions inconnues. Non seulement on n'a pas accès à leur code source, mais en
plus on ne connaît pas \emph{a priori} leur spécification. Une solution peut
être de prévoir un traitement particulier pour celles-ci (par exemple en leur
attribuant un type prédéfini), mais leur sémantique d'exécution peut être
complexe à exprimer car elles peuvent interagir avec le monde extérieur.

Certaines interagissent directement avec le système d'exploitation, comme les
fonctions d'ouverture ou d'écriture dans un fichier. D'autres modifient
totalement le mode d'exécution du programme. Par exemple
\texttt{pthread\_create(\&t, NULL, f, NULL)} lance l'exécution de
\texttt{f(NULL)} tout en continuant l'exécution de la fonction en cours
dans un fil d'exécution concurrent.

\paragraph{Extensions du langage}

\begin{figure}%{{{
    \centering
    \begin{minipage}{0.4\textwidth}
    \insertcode{packed1.c}
    \end{minipage}
    \begin{minipage}{0.4\textwidth}
    \begin{tikzpicture}
        [ base/.style={draw, rectangle, minimum height=5mm, minimum width=5mm}
        , faded/.style={fill=black!20}
        , padding/.style={base, faded}
        , data/.style={base}
        , bold/.style={line width=1.5pt}
        ]
        \node[data]                        at (  0, 0) {\texttt{a}};
        \node[padding, minimum width=15mm] at (  1, 0) { };
        \node[data,    minimum width=20mm] at (2.75, 0) {\texttt{b}};
        \node[data,    minimum width=10mm] at (4.25, 0) {\texttt{c}};
        \node[padding, minimum width=10mm] at (5.25, 0) { };
        \draw[bold] (-0.25, -0.25) -- ++(0, 0.5);
        \draw[bold] (1.75, -0.25) -- ++(0, 0.5);
        \draw[bold] (3.75, -0.25) -- ++(0, 0.5);
        \draw[bold] (5.75, -0.25) -- ++(0, 0.5);
    \end{tikzpicture}
    \end{minipage}

    \vspace{5mm}

    \begin{minipage}{0.4\textwidth}
    \insertcode{packed2.c}
    \end{minipage}
    \begin{minipage}{0.4\textwidth}
    \begin{tikzpicture}
        [ base/.style={draw, rectangle, minimum height=5mm, minimum width=5mm}
        , data/.style={base}
        ]
        \node[data]                        at (   0, 0) {\texttt{a}};
        \node[data,    minimum width=20mm] at (1.25, 0) {\texttt{b}};
        \node[data,    minimum width=10mm] at (2.75, 0) {\texttt{c}};
    \end{tikzpicture}
    \end{minipage}

    \caption{Utilisation de l'attribut non-standard \texttt{packed}}
    \label{fig:packed}
\end{figure}%}}}

Par exemple, la figure~\ref{fig:packed} démontre l'influence de l'attribut
\texttt{packed} (supporté par GCC) sur la compilation d'une structure. Sans
celui-ci, les champs sont alignés de manière à faciliter les accès à la mémoire,
par exemple en faisant démarrer les adresses de chaque champ sur un multiple de
4 octets (en gras). Cela nécessite d'introduire des octets de \emph{padding} (en
gris) qui ne sont pas utilisés. La taille totale de cette structure est donc de
12 octets.

Au contraire, l'utilisation de \texttt{packed} supprime totalement le
\emph{padding} et permet de diminuer alors la taille de la structure à 7 octets
seulement. Puisque \texttt{b} et \texttt{c} ne sont pas alignés, leur accès sera
fait de manière moins efficace.

De manière générale, les compilateurs permettent de personaliser finement le
code émis grâce à des extensions. Elles changent parfois le mode d'exécution des
programmes d'une manière subtile et pas toujours bien spécifiée ni documentée.

\section{Objectifs et contributions de la thèse}

Le but de ce travail est de définir et d'implanter des analyses statiques
\enquote{légères}sur le langage C, c'est-à-dire plus simples que les analyses de
valeurs par interprétation abstraite. Il est possible de réaliser celles-ci,
soit directement sur C, soit en utilisant un des langages intermédiaires
mentionnés dans la section~\ref{sec:lang-int}. Nous proposons d'utiliser
\newspeak pour analyser des propriétés de sécurité par typage sur du code non
avionique. Cela est pertinent pour les raisons suivantes:

\begin{itemize}

\item
Les types permettent de modéliser l'environnement d'exécution d'un
programme (ici, les paramètres d'appels système) avec un grain assez grand. Être
plus fin est difficile et nécessite de modéliser l'environnement.

\item
Réaliser des analyses syntaxiques sur \newspeak peut permettre de guider en
amont des analyses de valeurs plus précises (par exemple en choisissant des
domaines abstraits particuliers). Le typage de \newspeak est un premier pas dans
cette direction.

\end{itemize}

Nos contributions sont les suivantes:

\begin{itemize}

\item
Une première étape est de définir des restrictions sur le langage source. En
effet, le langage C comme \newspeak permettent des conversions non sûres entre
données, ce qui limite l'intérêt du typage. Cela revient à définir un
langage impératif avec un modèle mémoire de plus haut niveau, que l'on nomme
\langname.

\item
Sur ce langage on définit une sémantique opérationnelle, qui permet de raisonner
sur l'exécution des programmes. On profite du caractère structuré des états
mémoire pour exprimer cette sémantique en terme de lentilles
bidirectionnelles, permettant de décrire la modification en profondeur de la
mémoire.

\item
Au cœur de notre travail se trouve un système de types sûrs pour \langname,
ainsi que deux extensions validant l'approche typage pour résoudre notre
problème de base, qui est la vérification des accès aux pointeurs utilisateur.

\item
Notre formalisation est accompagnée d'un prototype basé sur \newspeak, ce qui
permet de de traiter des programmes écrits en C comme le noyau Linux. Ce
logiciel est disponible sous une license libre.

\end{itemize}

\section{Plan de la thèse}

Cette thèse est organisée en trois parties. La première décrit le contexte de
ces travaux, ainsi que les solutions existantes. La deuxième expose notre
solution, \langname, d'un point de vue théorique. La troisième rend compte de la
démarche expérimentale: comment la solution a été implantée et en quoi elle est
applicable en pratique.

\paragraph{Dans la partie~\ref{part:ctx},} on présente tout d'abord le
fonctionnement général d'un système d'exploitation. On y introduit aussi les
problèmes de manipulation de pointeurs contrôlés par l'utilisateur. Ceux-ci sont
centraux puisqu'on désire les restreindre. On fait ensuite un tour d'horizon des
techniques existantes permettant de traiter ce problème par analyse statique de
code source.

\paragraph{Dans la partie~\ref{part:lang},} on décrit notre solution: le langage
\langname{}. Sa syntaxe y est d'abord décrite, puis sa sémantique ainsi qu'un
système de types statiques. À ce niveau on a un bon support pour décrire des
analyses statiques sur un langage impératif. On propose alors deux extensions
indépendantes au système de types. La première consiste à bien typer les entiers
utilisés comme \emph{bitmasks}. La seconde capture les problèmes d'adressage
mémoire présents dans les systèmes d'exploitation. Pour ce faire, on ajoute des
pointeurs contrôlés par l'utilisateur à la sémantique et au système de types. À
chaque étape, c'est-à-dire avant et après ces ajouts, on établit une propriété
de sûreté de typage reliant la sémantique d'exécution aux types statiques.

\paragraph{Dans la partie~\ref{part:xp},} on documente la démarche expérimentale
associée à ces travaux. L'implantation du système de types sur le langage
\newspeak est d'abord décrite, à l'aide d'une variante de l'algorithme W de
Damas et Milner. La manière de compiler depuis du code C est également
présentée. Ensuite, on applique cette implantation à deux cas d'étude concrets
dans le noyau Linux. L'un est un bug ayant touché un pilote de carte graphique,
et l'autre un défaut dans l'implantation de la fonction \texttt{ptrace}. Dans
chaque cas, un pointeur dont la valeur est contrôlée par l'utilisateur crée un
problème de sécurité car un utilisateur malveillant peut lire ou écrire dans
l'espace mémoire réservé au noyau. En lançant notre prototype, l'analyse de la
version non corrigée lève une erreur alors que, dans la version corrigée, un
type correct est inféré. On montre ainsi que le système de types capture
précisément ce genre d'erreur de programmation.

% TODO veuve

On conclut enfin en décrivant les possibilités d'extension autant sur le point
théorique qu'expérimental.

% vim: spelllang=fr
